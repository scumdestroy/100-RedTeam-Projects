# So I don't exactly know what "banner exploit" is supposed to mean.  I tried to research some exploit involving a banner grab or maybe a "MOTD" type implant but nothing really nailed what I was looking for. 
# Therefore, I improved an earlier fingerprinting script that analyzes response content and various server headers to find a target's technology.
# Via some simple regex and a small list of common web tech, if it finds a match, it suggests search for said tech in an online exploit database (vulners, sploitus, exploit-db, whatever your preference, my friend).

# A script that takes a domain or IP address as an input and send a request to the target
# Headers are  then parsed to discover technologies used by the target

import requests
import sys
import argparse
import re
import json
import os
import time
import urllib3
import subprocess
import shlex
from bs4 import BeautifulSoup


# Disable SSL warnings
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Global variables
# List of headers to be ignored

IGNORE_HEADERS = [
    
    'X-Frame-Options',
    'X-XSS-Protection',
    'X-Content-Type-Options',
    'Content-Security-Policy',
    'Strict-Transport-Security',
    'X-Frame-Options']

# If script is run without arguments, print help and exit

if len(sys.argv) == 1:
    print('Usage: ' + sys.argv[0] + '<target>')
    sys.exit(1)
    
# Parse arguments

parser = argparse.ArgumentParser(description='A script that takes a domain or IP address as an input and send a request to the target. Headers are then parsed to discover technologies used by the target.')

parser.add_argument('target', help='Target domain or IP address')
parser.add_argument('-p', '--port', help='Target port', default=80)
parser.add_argument('-o', '--output', help='Output directory', default='./')
parser.add_argument('-s', '--ssl', help='Force SSL', action='store_true')
parser.add_argument('-u', '--useragent', help='User agent', default='Sentient Microwave Oven by HyperBaby Telegenetics Inc.')
parser.add_argument('-x', '--proxy', help='Proxy', default='')
parser.add_argument('-T', '--timeout', help='Timeout', default=10)
args = parser.parse_args()

target = sys.argv[1]
port = args.port
output = args.output
ssl = args.ssl
useragent = args.useragent
proxy = args.proxy
timeout = int(args.timeout)

search_terms = ["apache", "nginx", "tomcat", "spring", "wordpress", "symfony", "asp.net", "laravel", "rails", "drupal", "django", "flask", "catalyst"]

response = requests.get('http://' + target + ':' + port, verify=False, timeout=timeout, proxy=proxy, headers={'User-Agent': useragent})
if response.status_code == 200:
    print("Page retrieved successfully. Status code:", response.status_code)
    print("Interesting Headers:")
    # if header contains "Host", "Server", "Powered", "WAF", "Via", "Proxy", "Version", "Forwards", "Proto", "Envoy"  
    print(response.headers)
    print("Interesting Content:")   
    soup = BeautifulSoup(response.text, "html.parser")
    meta_tags = soup.find_all("meta")
    for tag in meta_tags:
        if "technology" in str(tag).lower():
            print(tag)
    response_text = response.text.lower()
    for term in search_terms:
       if term in response_text:
                print(f"Found '{term}' in the response.")
                print(f"Check here for potential exploits. https://vulners.com/search?query={term}")
    

    
else:
    print("Failed to retrieve the page. Status code:", response.status_code)

